<!DOCTYPE html>
<html>
<title>Resampling Point Clouds</title>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<head>
  <link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
  <!--Taken from "https://www.w3schools.com/w3css/tryit.asp?filename=tryw3css_templates_architect&stacked=h"-->
  
  <link rel="stylesheet" type="text/css" href="../static/styles/weblinks.css">
  <link rel="stylesheet" type="text/css" href="../static/styles/slideshows.css">
</head>


<body>


<!--Top navigation bar-->
<div class="w3-top">
  <div class="w3-bar w3-white w3-wide w3-padding w3-card">
    <a href={{ url_for("index") }} class="w3-bar-item w3-button"><b>Go back</b></a>

    <div class="w3-right w3-hide-small">
      <a href="#Background" class="w3-bar-item w3-button">Background</a>
      <a href="#Approaches" class="w3-bar-item w3-button">Approaches</a>
    </div>
  </div>
</div>


<!--Main body-->
<div class="w3-content w3-padding" style="max-width:1564px">
  <div class="w3-container w3-padding-32" id="Background">
    <h3 class="w3-border-bottom w3-border-grey w3-padding-16">Background</h3>

    <div class="w3-content w3-display-container", style="max-width:512px; max-height:512px">
      <div class="w3-content w3-display-container slideshow1" style="max-width:512px; max-height:512px">
        <img src="../static/images/resampling/plane.gif" alt="plane" style="max-width:512px; max-height:512px">
        <div class="w3-display-bottommiddle w3-medium w3-container" style="text-align:center">A plane from <a class="weblink" href="https://modelnet.cs.princeton.edu/">ModelNet</a>.</div>
      </div>
      <div class="w3-content w3-display-container slideshow1" style="max-width:512px; max-height:512px">
        <img src="../static/images/resampling/person.gif" alt="person" style="max-width:512px; max-height:512px">
        <div class="w3-display-bottommiddle w3-medium w3-container" style="text-align:center">A person from <a class="weblink" href="https://modelnet.cs.princeton.edu/">ModelNet</a>.</div>
      </div>
      <div class="w3-content w3-display-container slideshow1" style="max-width:512px; max-height:512px">
        <img src="../static/images/resampling/plant.gif" alt="plant" style="max-width:512px; max-height:512px">
        <div class="w3-display-bottommiddle w3-medium w3-container" style="text-align:center">A plant from <a class="weblink" href="https://modelnet.cs.princeton.edu/">ModelNet</a>.</div>
      </div>
      <button class="w3-button w3-black w3-display-bottomleft" onclick="plusSlides(-1, 0)">&#10094;</button>
      <button class="w3-button w3-black w3-display-bottomright" onclick="plusSlides(1, 0)">&#10095;</button>
    </div>

    <p>Point clouds are becoming one of the standard ways to store 3D data.
    They offer a sparse representation of the data which allows them to be
    more compact in storage (usually). However, because of this sparsity,
    often times the fine detail of the underlying surface is lost. The only
    geometry present in the data is a collection of points, unlike another
    data format such as meshes which contain information about points <i>and</i>
    faces. Due to this constraint, obtaining sparse representations of the
    surface while also being able to retain the finer features is of
    considerable interest. To tackle this, we try to resample accurately
    from the surface represented by the point cloud. This will allow use to
    construct arbitrary resolution point clouds given a single, sparse point
    cloud of the surface.</p>
  </div>

  <div class="w3-container w3-padding-32" id="Approaches">
      <h3 class="w3-border-bottom w3-border-grey w3-padding-16">Approaches</h3>
      <p>We set up a <a class="weblink" href="https://en.wikipedia.org/wiki/Autoencoder#Variational_autoencoder_(VAE)">
      variational autoencoder</a>-like framework and borrow ideas from
      <a class="weblink" href="https://arxiv.org/abs/1908.09186">this paper</a>.
      Essentially, the point cloud will pass as input into the network, which
      first predicts a <i>basis point set code</i> (latent representation)
      which is then decoded into an arbitrary resolution point cloud. We also
      have two sets of labels: (1) the latent representation of the mesh of
      the object and (2) the latent representation of the sparse point cloud.</p>

      <p>In more detail, we first construct a fixed basis point set for all
      samples. Then we pass an \( N \times 3 \) set of points into an encoder
      network. This then predicts a mean, \( \mu \), and "error", \( \epsilon \),
      which should represent our point cloud in an efficient way. From there,
      we pass the code into a decoder network which outputs an \( N^* \times 3 \)
      set of points. Note that we also only have to train one of the networks,
      e.g. the encoder or the decoder. If we optimize the encoder network, we
      construct our loss function using \( L^1 \) losses:
      \[ \lvert \mu - M \rvert + \lvert \epsilon - (M-E) \rvert, \]
      where \( \mu \) is the predicted mean, \( \epsilon \) is the predicted
      "error", \( M \) is the label for the mesh representation, and \( E \) is
      the label for the sparse point cloud.
  </div>
</div>


<!-- Footer -->
<footer class="w3-center w3-black w3-padding-16">
  <p>Joint work with <a class="weblink" href="http://cs.uky.edu/~jacobs/">Nathan Jacobs</a>
  and Hunter Blanton.</p>
</footer>


<script src="../static/scripts/slideshows.js"></script>


</body>


</html>
